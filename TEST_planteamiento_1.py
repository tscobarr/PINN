import tensorflow as tf  # Librer√≠a para aprendizaje profundo
import numpy as np  # Librer√≠a para manipulaci√≥n de matrices y c√°lculos num√©ricos
import matplotlib.pyplot as plt  # Librer√≠a para visualizaci√≥n de gr√°ficos
from matplotlib import rcParams  # Configuraci√≥n de gr√°ficos
import os  # Manejo de archivos y directorios
import pandas as pd  # Manejo de datos en formato tabular
os.environ["KERAS_BACKEND"] = "tensorflow"  # Define TensorFlow como backend de Keras
import keras_core as keras  # Librer√≠a de redes neuronales basada en TensorFlow
import time  # Para medir tiempos de ejecuci√≥n
import gc  # Para gesti√≥n de la memoria (recolecci√≥n de basura)
import psutil  # Para medir uso de CPU durante la ejecuci√≥n de experimentos

# Configuraci√≥n de reproducibilidad
keras.utils.set_random_seed(1234)  # Fija una semilla para asegurar resultados reproducibles
dtype = 'float32'  # Establece la precisi√≥n de los c√°lculos
keras.backend.set_floatx(dtype)  # Aplica el tipo de dato definido

# Definici√≥n de par√°metros globales
alpha = 1  # Par√°metro utilizado en las ecuaciones diferenciales
limInf = 0  # L√≠mite inferior del dominio de la soluci√≥n
limSup = np.pi  # L√≠mite superior del dominio de la soluci√≥n
iterations = 1500

def makeModel1(neurons, nLayers, activation):
    xVals = keras.layers.Input(shape=(1,), name='x_input', dtype=dtype)
    l1 = keras.layers.Dense(neurons, activation=activation, dtype=dtype)(xVals)
    for l in range(nLayers - 2):
        l1 = keras.layers.Dense(neurons, activation=activation, dtype=dtype)(l1)
    output = keras.layers.Dense(1, activation=activation, dtype=dtype)(l1)
    uModel = keras.Model(inputs=xVals, outputs=output, name='u_model')
    return uModel

class Loss1(keras.layers.Layer):
    def __init__(self, uModel, nPts, f, lambda0=1, lambda1=1, lambda2=1, limInf=0, limSup=np.pi, **kwargs):
        super(Loss1, self).__init__()
        self.uModel = uModel
        self.nPts = nPts
        self.f = f
        self.lambda0 = lambda0
        self.lambda1 = lambda1
        self.lambda2 = lambda2
        self.limInf = limInf
        self.limSup = limSup

    def call(self, inputs):
        x = tf.random.uniform([self.nPts], self.limInf, self.limSup, dtype=dtype)
        with tf.GradientTape(persistent=True) as t1:
            t1.watch(x)
            with tf.GradientTape(persistent=True) as t2:
                t2.watch(x)
                u = self.uModel(x, training=True)
            dux = t2.gradient(u, x)
        duxx = t1.gradient(dux, x)
        errorPDE = self.lambda0 * keras.ops.mean((-duxx + alpha * u - self.f(x)) ** 2)
        bc = self.lambda1 * self.uModel(np.array([self.limInf])) ** 2 + self.lambda2 * self.uModel(np.array([self.limSup])) ** 2
        return errorPDE + bc

class RelativeErrorCallback(tf.keras.callbacks.Callback):
    def __init__(self, uModel, exactU, nPts):
        super().__init__()
        self.uModel = uModel
        self.exactU = exactU
        self.nPts = nPts

    def on_epoch_end(self, epoch, logs=None):
        Sval = tf.experimental.numpy.linspace(0., np.pi, num=self.nPts*10, dtype=dtype)
        with tf.GradientTape(persistent=True) as t1:
            t1.watch(Sval)
            ueval_val = self.uModel(Sval)
            u_x_val = t1.gradient(ueval_val, Sval)
            u_e = self.exactU(Sval)
            ue_x = t1.gradient(u_e, Sval)
        del t1
        errorH01 = tf.reduce_mean((ue_x - u_x_val)**2)
        norm_exact = tf.reduce_mean(ue_x**2)
        relative_error = errorH01 / norm_exact
        logs['relative_error'] = relative_error

def makeLossModel1(uModel, nPts, f, lambda0=1, lambda1=1, lambda2=1, limInf=0, limSup=np.pi):
    xVals = keras.layers.Input(shape=(1,), name='x_input', dtype=dtype)
    loss_output = Loss1(uModel, nPts, f, lambda0, lambda1, lambda2, limInf, limSup)(xVals)
    lossModel = keras.Model(inputs=xVals, outputs=loss_output)
    return lossModel

def trickyLoss(yPred, yTrue):
    return yTrue

def crear_estructura_directorios(base_path="experimentos/PLANTEAMIENTO 1"):
    """
    Crea la estructura de directorios necesarios para almacenar los resultados de los experimentos.
    
    Par√°metros:
    - base_path (str): Ruta base donde se almacenar√°n los resultados.
    
    Retorna:
    - base_path (str): Ruta base de los experimentos.
    - graficas_path (str): Ruta donde se guardar√°n las gr√°ficas generadas.
    - mejores_path (str): Ruta espec√≠fica para almacenar los mejores resultados.
    - peores_path (str): Ruta espec√≠fica para almacenar los peores resultados.
    """
    graficas_path = os.path.join(base_path, "graficas")

    
    print("‚úî Directorios creados correctamente.")
    return base_path, graficas_path

def inicializar_excel(file_path="experimentos/PLANTEAMIENTO 1/resultados.xlsx"):
    """
    Crea un archivo Excel con la estructura inicial si no existe. 
    El archivo contendr√° tres hojas llamadas "Lote 1", "Lote 2" y "Lote 3",
    donde se almacenar√°n los resultados de los experimentos.

    Par√°metros:
    - file_path (str): Ruta del archivo Excel donde se guardar√°n los datos.

    Retorna:
    - file_path (str): Ruta del archivo Excel.
    """
    if not os.path.exists(file_path):
        writer = pd.ExcelWriter(file_path, engine='xlsxwriter')
        for i in range(1, 4):
            # Definir las columnas del archivo Excel
            df = pd.DataFrame(columns=[
                "Experimento", "neurons", "nLayers", "nPts", 
                "lambda0", "lambda1", "lambda2", "ERROR RELATIVO", "COSTO COMPUTACIONAL"
            ])
            df.to_excel(writer, sheet_name=f"Lote {i}", index=False)
        writer.close()
        print("‚úî Archivo Excel inicializado.")
    else:
        print("‚úî Archivo Excel ya existe.")
    
    return file_path

# Funci√≥n para obtener los hiperpar√°metros por defecto
def hiperparametros_por_defecto():
    """
    Devuelve un diccionario con valores predeterminados para los hiperpar√°metros de la PINN.

    Retorna:
    - dict: Diccionario con los hiperpar√°metros iniciales.
    """
    return {
        "neurons": 10,  # N√∫mero de neuronas por capa
        "nLayers": 5,  # N√∫mero de capas ocultas
        "nPts": 2000,  # N√∫mero de puntos en el dominio
        "lambda0": 5,  # Peso del error de la ecuaci√≥n diferencial
        "lambda1": 7,  # Peso del error en el contorno inferior
        "lambda2": 7   # Peso del error en el contorno superior
    }

# Funci√≥n para generar hiperpar√°metros aleatorios dentro de ciertos rangos
def randomizar_hiperparametros():
    """
    Genera un conjunto aleatorio de hiperpar√°metros dentro de ciertos rangos predefinidos.

    Retorna:
    - dict: Diccionario con hiperpar√°metros aleatorios.
    """
    return {
        "neurons": np.random.randint(4, 11),  # Neuronas entre 4 y 10
        "nLayers": np.random.randint(4, 6),  # Capas entre 4 y 5
        "nPts": np.random.randint(500, 3001),  # Puntos entre 500 y 3000
        "lambda0": np.random.randint(1, 11),  # Peso entre 1 y 10
        "lambda1": np.random.randint(4, 11),  # Peso entre 4 y 10
        "lambda2": np.random.randint(4, 11)   # Peso entre 4 y 10
    }

# Funci√≥n para definir la p√©rdida y entrenamiento
def train_PINN(hiperparametros, funcion_referencia, funcion_exacta):
    """
    Entrena la PINN con los hiperpar√°metros dados y la funci√≥n de referencia seleccionada.

    Tambi√©n mide el error relativo y el costo computacional en t√©rminos de uso de CPU.

    Par√°metros:
    - hiperparametros (dict): Diccionario con la configuraci√≥n de la red (neurons, nLayers, etc.).
    - funcion_referencia (funci√≥n): Funci√≥n f(x) en la ecuaci√≥n diferencial que la PINN debe aprender.
    - funcion_exacta (funci√≥n): Soluci√≥n exacta esperada para evaluar el error relativo.

    Retorna:
    - error_relativo (float): Error relativo entre la soluci√≥n de la PINN y la soluci√≥n exacta.
    - costo_computacional (float): Uso de CPU en porcentaje durante el entrenamiento.
    - history (tf.keras.callbacks.History): Historial del entrenamiento.
    - uModel (keras.Model): Modelo entrenado de la PINN.
    - xList (array): Puntos en el dominio donde se evalu√≥ la soluci√≥n.
    """

    # üìå Medir uso de CPU antes del entrenamiento
    cpu_usage_before = psutil.cpu_percent(interval=None)

    # üîπ Construcci√≥n del modelo de red neuronal PINN
    uModel = makeModel1(
        neurons=hiperparametros["neurons"],
        nLayers=hiperparametros["nLayers"],
        activation='tanh'
    )

    # # üìå Calcular valores exactos en los bordes (x = 0 y x = œÄ)
    # A = funcion_exacta(0)  # üìå Valor exacto en x = 0
    # B = funcion_exacta(np.pi)  # üìå Valor exacto en x = œÄ

    # üîπ Definir el modelo de p√©rdida basado en la ecuaci√≥n diferencial
    lossModel = makeLossModel1(
        uModel,
        hiperparametros["nPts"],
        funcion_referencia,  # üìå Funci√≥n f(x)
        hiperparametros["lambda0"],
        hiperparametros["lambda1"],
        hiperparametros["lambda2"],
        limInf, limSup,
        # A, B  # üìå Se pasan los valores exactos en los extremos
    )

    # üìå Configurar el optimizador y la funci√≥n de p√©rdida (trickyLoss)
    optimizer = keras.optimizers.Adam(learning_rate=1e-3)
    lossModel.compile(optimizer=optimizer, loss=trickyLoss)

    relative_error_callback = RelativeErrorCallback(uModel, funcion_referencia, hiperparametros["nPts"])
    # üìå Entrenamiento de la PINN
    history = lossModel.fit(
        np.array([1.]), np.array([1.]), epochs=iterations, verbose=0, callbacks=[relative_error_callback]
    )

    # üîπ Generar puntos en el dominio para evaluar la soluci√≥n entrenada
    xList = np.array([np.pi / 1000 * i for i in range(1000)])

    # üîπ Evaluar la soluci√≥n aproximada y compararla con la soluci√≥n exacta
    error_relativo = history.history['relative_error'][-1].numpy()

    # üìå Medir uso de CPU despu√©s del entrenamiento
    cpu_usage_after = psutil.cpu_percent(interval=None)
    cpu_cost = cpu_usage_after - cpu_usage_before  # üìå C√°lculo del costo computacional

    # üîπ Liberar memoria
    keras.backend.clear_session()
    gc.collect()

    return error_relativo, cpu_cost, history, uModel, xList  # üìå Retornar m√©tricas clave y el modelo entrenado

def ejecutar_experimentos(func, n_experimentos=45):  
    """
    Ejecuta m√∫ltiples experimentos de entrenamiento de la PINN con diferentes hiperpar√°metros.

    Se generan varios lotes de experimentos con diferentes configuraciones de hiperpar√°metros 
    para evaluar la estabilidad y precisi√≥n del modelo en distintas condiciones.

    Par√°metros:
    - func (lista de tuplas): Lista de funciones de referencia y soluciones exactas esperadas.
      Cada elemento es una tupla (funci√≥n_f, funci√≥n_u_exacta).
    - n_experimentos (int, opcional): N√∫mero total de experimentos a ejecutar. Por defecto, 45.

    Retorna:
    - resultados (lista): Lista de resultados de los experimentos, donde cada elemento contiene:
      [ID, neurons, nLayers, nPts, lambda0, lambda1, lambda2, error_relativo, costo_computacional, 
      modelo entrenado, historial, soluci√≥n exacta, xList].
    """

    # üìå Determinar la cantidad de lotes basada en el n√∫mero de experimentos
    num_lotes = min(len(func), n_experimentos)  
    lote_size = n_experimentos // num_lotes  # üìå N√∫mero de experimentos por lote

    # üìå Generar hiperpar√°metros aleatorios para cada lote (excepto el primero, que es por defecto)
    hiperparametros_aleatorios = [randomizar_hiperparametros() for _ in range(lote_size - 1)]  

    # üìå Lista para almacenar resultados
    resultados = []

    for i in range(num_lotes):
        # üîπ Seleccionar la funci√≥n de referencia y la soluci√≥n exacta
        f_rhs, exactU = func[i]  
        print(f"\nüîπ Ejecutando Lote {i+1}...\n")

        for j in range(lote_size):
            experimento_id = i * lote_size + j + 1
            print(f"   ‚ñ∂ Ejecutando experimento {experimento_id}...")

            # üîπ Para el primer experimento de cada lote, usar hiperpar√°metros por defecto
            if j == 0:
                hiperparametros = hiperparametros_por_defecto()
            else:
                hiperparametros = hiperparametros_aleatorios[j - 1]

            # üîπ Entrenar la PINN con la configuraci√≥n actual
            error_relativo, costo_computacional, history, uModel, xList = train_PINN(hiperparametros, f_rhs, exactU)

            # Imprimir resultados del experimento
            print(f"      üîπ Completado - Error relativo: {error_relativo:.5f}, Uso CPU: {costo_computacional:.2f}%")

            # üìå Guardar los resultados del experimento
            resultados.append([
                experimento_id, 
                hiperparametros["neurons"], 
                hiperparametros["nLayers"], 
                hiperparametros["nPts"], 
                hiperparametros["lambda0"], 
                hiperparametros["lambda1"], 
                hiperparametros["lambda2"], 
                error_relativo, 
                costo_computacional,  
                uModel,   
                history,  
                exactU,   
                xList     
            ])

    return resultados

# üîπ Definimos las funciones de referencia y sus soluciones exactas en un diccionario
funciones_experimentos = [
    {
        'fRhs': lambda x: (alpha + 4) * keras.ops.sin(2 * x),
        'exactU': lambda x: keras.ops.sin(2 * x),
        'title': 'sin(2x)'
    },
    {
        'fRhs': lambda x: 17/4 * keras.ops.sin(2 * x) * keras.ops.cos(x / 2) + 2 * keras.ops.sin(x / 2) * keras.ops.cos(2 * x) + alpha * keras.ops.sin(2 * x) * keras.ops.cos(x / 2),
        'exactU': lambda x: keras.ops.sin(2 * x) * keras.ops.cos(x / 2),
        'title': 'sin(2x) * cos(x/2)'
    },
    {
        'fRhs': lambda x: - 2 * (6 * x**2 - 6 * np.pi * x + np.pi**2) + alpha * (x**2 * (x - np.pi)**2),
        'exactU': lambda x: x**2 * (x - np.pi)**2,
        'title': 'x^2 * (x - pi)^2'
    }
]

# üîπ Convertir el diccionario en una lista de tuplas
func = [(exp['fRhs'], exp['exactU']) for exp in funciones_experimentos]

def guardar_resultados_excel(resultados, file_path="experimentos/PLANTEAMIENTO 1/resultados.xlsx"):
    """
    Guarda los resultados de los experimentos en un archivo Excel, separando por lote.

    Cada hoja del archivo representa un conjunto de experimentos con la misma funci√≥n de referencia.

    Par√°metros:
    - resultados (lista): Lista de resultados obtenidos de `ejecutar_experimentos`.
    - file_path (str, opcional): Ruta del archivo Excel donde se guardar√°n los datos.

    Retorna:
    - None
    """
    from openpyxl import load_workbook

    # üìå Cargar o crear un archivo Excel
    if os.path.exists(file_path):
        writer = pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay')
    else:
        writer = pd.ExcelWriter(file_path, engine='xlsxwriter')

    # üìå Recorrer cada experimento y guardarlo en su respectiva hoja
    for i, experimento in enumerate(funciones_experimentos):
        lote_resultados = [
            [r[j] for j in range(9)] for r in resultados if (r[0] - 1) // (len(resultados) // len(funciones_experimentos)) == i
        ]

        df = pd.DataFrame(
            lote_resultados,
            columns=["Experimento", "neurons", "nLayers", "nPts", 
                     "lambda0", "lambda1", "lambda2", "ERROR RELATIVO", "COSTO COMPUTACIONAL (CPU %)"]
        )

        # Reemplazar caracteres inv√°lidos en el nombre de la hoja
        sheet_name = experimento['title'].replace('*', '').replace('/', '').replace('\\', '').replace('[', '').replace(']', '').replace(':', '').replace('?', '')

        df.to_excel(writer, sheet_name=sheet_name, index=False)

    # üìå Guardar los cambios
    writer.close()
    print(f"‚úî Resultados guardados en {file_path} con las funciones correctamente organizadas.")

def plot_results(uModel, history, exactU, xList, exp_folder):
    """
    Genera y guarda las gr√°ficas de un experimento, asegurando que se use la funci√≥n exacta correcta.
    """
    rcParams['font.family'] = 'serif'
    rcParams['font.size'] = 14
    rcParams['legend.fontsize'] = 12
    rcParams['mathtext.fontset'] = 'cm'
    rcParams['axes.labelsize'] = 14

    # Comparaci√≥n de la soluci√≥n exacta y la aproximada
    fig, ax = plt.subplots()
    plt.plot(xList, uModel(xList), color='b', label="u_approx")
    plt.plot(xList, exactU(xList), color='m', label="u_exact")  # ‚úÖ Se asegura que usa la funci√≥n exacta correcta
    plt.legend()
    ax.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "comparacion_u.png"), dpi=300)
    plt.close()

    # Evoluci√≥n de la p√©rdida y error relativo
    fig, ax1 = plt.subplots()
    ax1.plot(history.history['loss'], color='g', label="Loss")
    ax1.set_xscale('log')
    ax1.set_yscale('log')
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Loss", color='g')

    ax2 = ax1.twinx()
    ax2.plot(history.history['loss'], color='b', label="Relative Error")  # üìå Se debe reemplazar por la m√©trica correcta
    ax2.set_ylabel("Relative Error", color='b')

    ax1.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "error_vs_loss.png"), dpi=300)
    plt.close()

    # Error relativo a lo largo de las √©pocas
    fig, ax = plt.subplots()
    plt.plot(history.history['loss'], color='b', label="Relative Error")  # üìå Se debe ajustar con la m√©trica real
    ax.set_xscale('log')
    plt.legend()
    ax.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "error_relativo.png"), dpi=300)
    plt.close()

def seleccionar_experimentos(resultados):
    """
    Selecciona los experimentos que se van a graficar y los organiza en una lista.

    Par√°metros:
    - resultados (lista): Lista de resultados obtenidos en `ejecutar_experimentos`.

    Retorna:
    - todos_experimentos (lista): Lista de experimentos seleccionados para graficar.
    """

    # üìå Determinar la cantidad de lotes seg√∫n los experimentos disponibles
    num_lotes = min(len(func), len(resultados))  
    lote_size = max(1, len(resultados) // num_lotes)  

    # üìå Lista para almacenar los experimentos seleccionados
    todos_experimentos = []

    for i in range(num_lotes):
        # üîπ Seleccionar experimentos de este lote
        lote_resultados = [r for r in resultados if (r[0] - 1) // lote_size == i]
        todos_experimentos.extend(lote_resultados)

    print("‚úî Experimentos listos para generar gr√°ficas.")
    return todos_experimentos

def generar_graficas(experimentos, resultados, graficas_path):
    """
    Genera y guarda las gr√°ficas de los experimentos en una carpeta espec√≠fica.

    Cada experimento tendr√° su propia carpeta donde se guardar√°n:
    - La comparaci√≥n entre la soluci√≥n exacta y la aproximada.
    - La evoluci√≥n de la p√©rdida durante el entrenamiento.
    - La evoluci√≥n del error relativo.
    - La comparaci√≥n entre la p√©rdida y el error relativo.

    Par√°metros:
    - experimentos (lista): Lista de experimentos seleccionados para graficar.
    - resultados (lista): Lista de resultados obtenidos en `ejecutar_experimentos`.
    - graficas_path (str): Ruta donde se almacenar√°n las gr√°ficas.

    Retorna:
    - None
    """

    for exp in experimentos:
        exp_id = exp[0]  # ID del experimento
        exp_folder = os.path.join(graficas_path, f"Experimento_{exp_id}")
        os.makedirs(exp_folder, exist_ok=True)  # üìå Crear carpeta del experimento si no existe

        # üîπ Extraer datos del experimento correspondiente
        uModel, history, exactU, xList = resultados[exp_id - 1][9:]

        # üìå Generar las gr√°ficas
        plot_results(uModel, history, exactU, xList, exp_folder)

    print(f"‚úî Todas las gr√°ficas generadas en {graficas_path}.")

# üîπ Paso 1: Configuraci√≥n inicial (Crear directorios y archivo Excel)
print("üìÇ Configurando estructura de almacenamiento...")
base_path, graficas_path = crear_estructura_directorios()
excel_file = inicializar_excel()

# üîπ Paso 2: Ejecutar experimentos
print("üöÄ Ejecutando experimentos...")
resultados_experimentos = ejecutar_experimentos(func)

# üîπ Paso 3: Seleccionar los experimentos para graficar
print("üìä Seleccionando experimentos para generaci√≥n de gr√°ficas...")
experimentos_seleccionados = seleccionar_experimentos(resultados_experimentos)

# üîπ Paso 4: Generar todas las gr√°ficas
print("üìà Generando gr√°ficas...")
generar_graficas(experimentos_seleccionados, resultados_experimentos, graficas_path)

# üîπ Paso 5: Guardar los resultados en Excel
print("üíæ Guardando resultados en archivo Excel...")
guardar_resultados_excel(resultados_experimentos)

print("‚úÖ Proceso finalizado exitosamente.")