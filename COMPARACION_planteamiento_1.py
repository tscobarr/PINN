import tensorflow as tf  # LibrerÃ­a para aprendizaje profundo
import numpy as np  # LibrerÃ­a para manipulaciÃ³n de matrices y cÃ¡lculos numÃ©ricos
import matplotlib.pyplot as plt  # LibrerÃ­a para visualizaciÃ³n de grÃ¡ficos
from matplotlib import rcParams  # ConfiguraciÃ³n de grÃ¡ficos
import os  # Manejo de archivos y directorios
import pandas as pd  # Manejo de datos en formato tabular
os.environ["KERAS_BACKEND"] = "tensorflow"  # Define TensorFlow como backend de Keras
import keras_core as keras  # LibrerÃ­a de redes neuronales basada en TensorFlow
import time  # Para medir tiempos de ejecuciÃ³n
import gc  # Para gestiÃ³n de la memoria (recolecciÃ³n de basura)
import psutil  # Para medir uso de CPU durante la ejecuciÃ³n de experimentos
import time
import re 

# ConfiguraciÃ³n de reproducibilidad
keras.utils.set_random_seed(1234)  # Fija una semilla para asegurar resultados reproducibles
dtype = 'float32'  # Establece la precisiÃ³n de los cÃ¡lculos
keras.backend.set_floatx(dtype)  # Aplica el tipo de dato definido

# DefiniciÃ³n de parÃ¡metros globales
alpha = 1  # ParÃ¡metro utilizado en las ecuaciones diferenciales
limInf = 0  # LÃ­mite inferior del dominio de la soluciÃ³n
limSup = np.pi  # LÃ­mite superior del dominio de la soluciÃ³n
iterations = 3 # epocas por experimento


def makeModel(neurons, nLayers, activation):
    """
    Construye un modelo de red neuronal profunda basado en una PINN (Physics-Informed Neural Network).
    
    ParÃ¡metros:
    - neurons (int): NÃºmero de neuronas en cada capa oculta.
    - nLayers (int): NÃºmero total de capas en la red (incluyendo la capa de entrada y salida).
    - activation (str): FunciÃ³n de activaciÃ³n utilizada en las capas ocultas.

    Retorna:
    - model (keras.Model): Modelo de red neuronal compilado.
    """
    # Capa de entrada con una Ãºnica neurona para recibir valores de 'x'
    xVals = keras.layers.Input(shape=(1,), name='x_input', dtype='float32')

    # Primera capa oculta
    l1 = keras.layers.Dense(neurons, activation=activation, dtype='float32')(xVals)

    # Capas ocultas intermedias (se asegura que haya al menos una capa oculta)
    for _ in range(max(1, nLayers - 2)):  
        l1 = keras.layers.Dense(neurons, activation=activation, dtype='float32')(l1)

    # Capa de salida con una Ãºnica neurona (salida escalar)
    output = keras.layers.Dense(1, activation=activation, dtype='float32')(l1)

    # ConstrucciÃ³n del modelo
    return keras.Model(inputs=xVals, outputs=output, name='u_model')
class Loss1(keras.layers.Layer):
    def __init__(self, uModel, nPts, f, lambda0=1, lambda1=1, lambda2=1, limInf=0, limSup=np.pi, A=0, B=0, **kwargs):
        super(Loss1, self).__init__()
        self.uModel = uModel
        self.nPts = nPts
        self.f = f
        self.lambda0 = lambda0
        self.lambda1 = lambda1
        self.lambda2 = lambda2
        self.limInf = limInf
        self.limSup = limSup
        self.A = A
        self.B = B

    def call(self, inputs):
        """
        EvalÃºa la funciÃ³n de pÃ©rdida en el dominio y los bordes.
        """

        # ğŸ”¹ Convertir la entrada `inputs` a `float32`
        x = tf.cast(inputs, tf.float32)  # âœ… Convertimos explÃ­citamente a `float32`

        # ğŸ”¹ Muestreo de puntos en el dominio con `float32`
        x_samples = tf.random.uniform([self.nPts], self.limInf, self.limSup, dtype=tf.float32)

        with tf.GradientTape(persistent=True) as t1:
            t1.watch(x_samples)
            with tf.GradientTape(persistent=True) as t2:
                t2.watch(x_samples)
                u = self.uModel(x_samples, training=True)  # âœ… Se mantiene en `float32`
            dux = t2.gradient(u, x_samples)
        duxx = t1.gradient(dux, x_samples)

        # ğŸ”¹ Asegurar `float32` en todos los cÃ¡lculos
        alpha_cast = tf.cast(alpha, tf.float32)
        f_x = tf.cast(self.f(x_samples), tf.float32)

        # ğŸ”¹ CÃ¡lculo del error de la ecuaciÃ³n diferencial
        errorPDE = self.lambda0 * keras.ops.mean(tf.cast((-duxx + alpha_cast * u - f_x) ** 2, tf.float32))

        # ğŸ”¹ Condiciones de contorno con `float32`
        bc = self.lambda1 * keras.ops.mean((self.uModel(tf.cast([self.limInf], tf.float32)) - tf.cast(self.A, tf.float32)) ** 2) + \
            self.lambda2 * keras.ops.mean((self.uModel(tf.cast([self.limSup], tf.float32)) - tf.cast(self.B, tf.float32)) ** 2)

        # âœ… Convertir la salida a tensor para evitar errores en Keras
        return tf.convert_to_tensor(errorPDE + bc, dtype=tf.float32)



class RelativeErrorCallback(tf.keras.callbacks.Callback):
    """
    Callback personalizado para calcular el error relativo en norma HÂ¹ al final de cada Ã©poca.

    Esta implementaciÃ³n mide la diferencia en la funciÃ³n y su derivada, 
    proporcionando una mejor evaluaciÃ³n de la precisiÃ³n del modelo.

    ParÃ¡metros:
    - uModel (keras.Model): Modelo de la PINN entrenado.
    - exactU (funciÃ³n): FunciÃ³n de la soluciÃ³n exacta.
    - nPts (int): NÃºmero de puntos en el dominio para evaluaciÃ³n.
    """

    def __init__(self, uModel, exactU, nPts):
        super().__init__()
        self.uModel = uModel
        self.exactU = exactU
        self.nPts = nPts

    def on_epoch_end(self, epoch, logs=None):
        """
        Se ejecuta al final de cada Ã©poca y calcula el error relativo en norma HÂ¹.
        """

        # ğŸ”¹ Generamos puntos en el dominio
        Sval = tf.experimental.numpy.linspace(0., np.pi, num=self.nPts * 10, dtype=dtype)

        # ğŸ”¹ Calculamos derivadas con `GradientTape`
        with tf.GradientTape(persistent=True) as t1:
            t1.watch(Sval)
            ueval_val = self.uModel(Sval)  # SoluciÃ³n aproximada de la PINN
            u_x_val = t1.gradient(ueval_val, Sval)  # Derivada de la soluciÃ³n aproximada
            u_e = self.exactU(Sval)  # SoluciÃ³n exacta
            ue_x = t1.gradient(u_e, Sval)  # Derivada de la soluciÃ³n exacta
        del t1  # Liberar memoria

        # ğŸ”¹ CÃ¡lculo del error relativo en norma HÂ¹
        errorH01 = tf.reduce_mean((ue_x - u_x_val) ** 2)
        norm_exact = tf.reduce_mean(ue_x ** 2)
        relative_error = errorH01 / norm_exact

        # ğŸ”¹ Guardamos el error relativo en los logs del entrenamiento
        logs['relative_error'] = relative_error


def makeLossModel1(uModel, nPts, f, lambda0=1, lambda1=1, lambda2=1, limInf=0, limSup=np.pi, A=0, B=0,):
    """
    Crea un modelo de pÃ©rdida basado en la funciÃ³n `Loss1` para entrenar la PINN.

    En lugar de entrenar la PINN directamente, este modelo de pÃ©rdida se construye para 
    minimizar la ecuaciÃ³n diferencial y las condiciones de contorno en cada iteraciÃ³n.

    ParÃ¡metros:
    - uModel (keras.Model): Modelo de la PINN que se estÃ¡ entrenando.
    - nPts (int): NÃºmero de puntos de muestreo en el dominio.
    - f (funciÃ³n): FunciÃ³n de referencia f(x) en la ecuaciÃ³n diferencial.
    - lambda0 (float, opcional): Peso del tÃ©rmino de la ecuaciÃ³n diferencial (PDE). Por defecto, 1.
    - lambda1 (float, opcional): Peso de la condiciÃ³n de contorno en el lÃ­mite inferior. Por defecto, 1.
    - lambda2 (float, opcional): Peso de la condiciÃ³n de contorno en el lÃ­mite superior. Por defecto, 1.
    - limInf (float, opcional): LÃ­mite inferior del dominio. Por defecto, 0.
    - limSup (float, opcional): LÃ­mite superior del dominio. Por defecto, Ï€.

    Retorna:
    - lossModel (keras.Model): Modelo de pÃ©rdida para la PINN.
    """
    # Entrada ficticia para alimentar el modelo de pÃ©rdida
    xVals = keras.layers.Input(shape=(1,), name='x_input', dtype=tf.float32)

    # Aplicamos la capa personalizada `Loss1` que calcula la pÃ©rdida de la ecuaciÃ³n diferencial
    loss_output = Loss1(uModel, nPts, f, lambda0, lambda1, lambda2, limInf, limSup,A, B,)(xVals)

    # ConstrucciÃ³n del modelo de pÃ©rdida
    lossModel = keras.Model(inputs=xVals, outputs=loss_output)
    
    return lossModel

def trickyLoss(yPred, yTrue):
    """
    FunciÃ³n de pÃ©rdida ficticia utilizada en el entrenamiento de la PINN.

    Dado que la pÃ©rdida real es calculada por la capa `Loss1`, esta funciÃ³n 
    simplemente devuelve `yTrue` sin hacer ningÃºn cÃ¡lculo. 

    ParÃ¡metros:
    - yPred (tensor): PredicciÃ³n del modelo de pÃ©rdida (`lossModel`).
    - yTrue (tensor): Etiqueta objetivo (se ignora).

    Retorna:
    - yTrue (tensor): Se devuelve sin modificaciones.
    """
    return yTrue


## INICIALIZAR CARPETAS Y EXCEL ## 


def crear_estructura_directorios(base_path="resultados/PLANTEAMIENTO 1"):
    """
    Crea la estructura de directorios necesarios para almacenar los resultados de los resultados.

    ParÃ¡metros:
    - base_path (str): Ruta base donde se almacenarÃ¡n los resultados.

    Retorna:
    - base_path (str): Ruta base de los experimentos.
    - graficas_path (str): Ruta donde se guardarÃ¡n las grÃ¡ficas generadas.
    """

    try:
        # ğŸ“Œ Verificar si la ruta ya existe
        if os.path.exists(base_path):
            if os.path.isfile(base_path):
                print(f"âš ï¸ Advertencia: '{base_path}' es un archivo, no un directorio. Se eliminarÃ¡ y se crearÃ¡ como carpeta.")
                os.remove(base_path)  
                os.makedirs(base_path)  
            else:
                print(f"âœ” La carpeta '{base_path}' ya existe.")
        else:
            os.makedirs(base_path)  

        # ğŸ“Œ Crear subdirectorio para grÃ¡ficas
        graficas_path = os.path.join(base_path, "graficas")
        os.makedirs(graficas_path, exist_ok=True)

        print("âœ” Directorios creados correctamente.")
        return base_path, graficas_path

    except Exception as e:
        print(f"âŒ Error al crear directorios: {e}")
        return None, None  # Retornar None en caso de error


def inicializar_excel(file_path="resultados/PLANTEAMIENTO 1/resultados.xlsx"):
    """
    Crea un archivo Excel con la estructura inicial si no existe. 
    El archivo contendrÃ¡ tres hojas llamadas "Lote 1", "Lote 2" y "Lote 3",
    donde se almacenarÃ¡n los resultados de los experimentos.

    ParÃ¡metros:
    - file_path (str): Ruta del archivo Excel donde se guardarÃ¡n los datos.

    Retorna:
    - file_path (str): Ruta del archivo Excel.
    """
    if not os.path.exists(file_path):
        writer = pd.ExcelWriter(file_path, engine='xlsxwriter')
        for i in range(1, 4):
            # Definir las columnas del archivo Excel
            df = pd.DataFrame(columns=[
                "Experimento", "neurons", "nLayers", "nPts", 
                "lambda0", "lambda1", "lambda2", "ERROR RELATIVO", "COSTO COMPUTACIONAL"
            ])
        writer.close()
        print("âœ” Archivo Excel inicializado.")
    else:
        print("âœ” Archivo Excel ya existe.")
    
    return file_path


# FunciÃ³n para obtener los hiperparÃ¡metros por defecto
def hiperparametros_por_defecto_1():
    """
    Devuelve un diccionario con valores optimos para los hiperparÃ¡metros de la PINN que fueron determinados
    experimentalmente.

    Retorna:
    - dict: Diccionario con los hiperparÃ¡metros iniciales.
    """
    return {
        "neurons": 8,  # NÃºmero de neuronas por capa
        "nLayers": 5,  # NÃºmero de capas ocultas
        "nPts": 1608,  # NÃºmero de puntos en el dominio
        "lambda0": 1,  # Peso del error de la ecuaciÃ³n diferencial
        "lambda1": 5,  # Peso del error en el contorno inferior
        "lambda2": 4   # Peso del error en el contorno superior
    }


def medir_costo_computacional(funcion_entrenamiento, *args, **kwargs):
    """
    Esta funciÃ³n mide el porcentaje de uso de CPU y el tiempo total transcurrido durante la ejecuciÃ³n de una funciÃ³n especÃ­fica. 
    Su propÃ³sito es evaluar el costo computacional de un proceso en tÃ©rminos de utilizaciÃ³n del procesador.

    Para lograrlo, la funciÃ³n realiza las siguientes mediciones antes y despuÃ©s de la ejecuciÃ³n de la funciÃ³n objetivo:
    1. **Tiempo de CPU utilizado**: Representa el tiempo total que la CPU ha dedicado activamente a ejecutar la funciÃ³n, 
    sin contar periodos de inactividad o espera.
    2. **Tiempo real transcurrido**: Es el tiempo total que ha pasado desde el inicio hasta el final de la ejecuciÃ³n de la funciÃ³n.
    3. **NÃºmero de nÃºcleos lÃ³gicos del procesador**: Determina la capacidad total del sistema para distribuir la carga de trabajo.

    El cÃ¡lculo del uso de CPU se realiza dividiendo el tiempo de CPU utilizado entre el producto del tiempo real transcurrido 
    y el nÃºmero de nÃºcleos lÃ³gicos. Esto permite normalizar la mediciÃ³n para obtener un valor porcentual que refleja 
    quÃ© tanto del poder total del procesador fue utilizado durante la ejecuciÃ³n.


    ParÃ¡metros:
    - funcion_entrenamiento (funciÃ³n): La funciÃ³n que queremos medir.
    - *args: Argumentos posicionales de la funciÃ³n.
    - **kwargs: Argumentos con nombre de la funciÃ³n.

    Retorna:
    - cpu_percent (float): Porcentaje medio de CPU usado durante la ejecuciÃ³n.
    - elapsed_time (float): Tiempo total transcurrido en segundos.
        
    """



    process = psutil.Process(os.getpid())  # Obtener el proceso actual

    # ğŸ”¹ Medir el tiempo de CPU y tiempo real antes del entrenamiento
    cpu_times_before = process.cpu_times()
    time_before = time.time()

    # ğŸ”¹ Ejecutar la funciÃ³n de entrenamiento con sus argumentos
    funcion_entrenamiento(*args, **kwargs)

    # ğŸ”¹ Medir el tiempo de CPU y tiempo real despuÃ©s del entrenamiento
    cpu_times_after = process.cpu_times()
    time_after = time.time()

    # ğŸ”¹ Calcular tiempo total de CPU consumido
    cpu_time_used = (cpu_times_after.user + cpu_times_after.system) - (cpu_times_before.user + cpu_times_before.system)

    # ğŸ”¹ Calcular tiempo real transcurrido
    elapsed_time = time_after - time_before

    # ğŸ”¹ Calcular porcentaje de CPU usado
    num_cpus = psutil.cpu_count(logical=True)  # NÃºmero de nÃºcleos lÃ³gicos

    cpu_percent = (cpu_time_used / (elapsed_time * num_cpus)) * 100 if elapsed_time > 0 else 0 

    return cpu_percent, elapsed_time


def train_PINN(hiperparametros, funcion_referencia, funcion_exacta):
    """
    Entrena la PINN con los hiperparÃ¡metros dados y la funciÃ³n de referencia seleccionada.

    TambiÃ©n mide el error relativo y el costo computacional en tÃ©rminos de uso de CPU.

    ParÃ¡metros:
    - hiperparametros (dict): Diccionario con la configuraciÃ³n de la red (neurons, nLayers, etc.).
    - funcion_referencia (funciÃ³n): FunciÃ³n f(x) en la ecuaciÃ³n diferencial que la PINN debe aprender.
    - funcion_exacta (funciÃ³n): SoluciÃ³n exacta esperada para evaluar el error relativo.

    Retorna:
    - error_relativo (float): Error relativo entre la soluciÃ³n de la PINN y la soluciÃ³n exacta.
    - costo_computacional (float): Uso de CPU en porcentaje durante el entrenamiento.
    - history (tf.keras.callbacks.History): Historial del entrenamiento.
    - uModel (keras.Model): Modelo entrenado de la PINN.
    - xList (array): Puntos en el dominio donde se evaluÃ³ la soluciÃ³n.
    """

    # ğŸ”¹ Medir tiempo de CPU antes del entrenamiento
    process = psutil.Process(os.getpid())

    # ğŸ”¹ ConstrucciÃ³n del modelo de red neuronal PINN
    uModel = makeModel(
        neurons=hiperparametros["neurons"],
        nLayers=hiperparametros["nLayers"],
        activation='tanh'
    )

    # ğŸ“Œ Calcular valores exactos en los bordes (x = 0 y x = Ï€)
    A = 0 #funcion_exacta(tf.convert_to_tensor(0.0, dtype=dtype))  # ğŸ“Œ Se asegura de que x sea un tensor
    B = 0 #funcion_exacta(tf.convert_to_tensor(np.pi, dtype=dtype)) 

    # ğŸ”¹ Definir el modelo de pÃ©rdida basado en la ecuaciÃ³n diferencial
    lossModel = makeLossModel1(
        uModel,
        hiperparametros["nPts"],
        funcion_referencia,
        hiperparametros["lambda0"],
        hiperparametros["lambda1"],
        hiperparametros["lambda2"],
        limInf, limSup,
        A, B  # ğŸ“Œ Se pasan los valores exactos en los extremos
    )

    # ğŸ“Œ Configurar el optimizador y la funciÃ³n de pÃ©rdida
    optimizer = keras.optimizers.Adam(learning_rate=1e-3)
    lossModel.compile(optimizer=optimizer, loss=trickyLoss)

    relative_error_callback = RelativeErrorCallback(uModel, funcion_exacta, hiperparametros["nPts"]) #CAMBIO REALIZADO
    
    # ğŸ“Œ Entrenamiento de la PINN
    history = lossModel.fit(
        np.array([1.]), np.array([1.]), epochs=iterations, verbose=0, callbacks=[relative_error_callback]
    )


    # ğŸ”¹ Generar puntos en el dominio para evaluar la soluciÃ³n entrenada
    xList = np.array([np.pi / 1000 * i for i in range(1000)])

    # ğŸ”¹ Evaluar la soluciÃ³n aproximada y compararla con la soluciÃ³n exacta
    error_relativo = history.history['relative_error'][-1].numpy()

    # ğŸ”¹ Calcular tiempo total de CPU usado (sumando usuario y sistema)
    cpu_cost, _ = medir_costo_computacional(
        lossModel.fit, np.array([1.]), np.array([1.]), epochs=iterations, verbose=0, callbacks=[relative_error_callback]
    )

    # ğŸ”¹ Liberar memoria
    keras.backend.clear_session()
    gc.collect()

    return error_relativo, cpu_cost, history, uModel, xList

def ejecutar_entrenamientos(func, n_resultados=6):  
    """
    Ejecuta mÃºltiples veces el entrenamiento de la PINN con hiperparÃ¡metros Ã³ptimos.

    Se generan los resultados de la PINN despuÃ©s de cada entrenamiento.

    ParÃ¡metros:
    - func (lista de tuplas): Lista de funciones de referencia y soluciones exactas esperadas.
      Cada elemento es una tupla (funciÃ³n_f, funciÃ³n_u_exacta).
    - n_resultados (int, opcional): NÃºmero total de entrenamientos a ejecutar. Por defecto, 45.

    Retorna:
    - resultados (lista): Lista de resultados de los entrenamientos, donde cada elemento contiene:
      [ID, neurons, nLayers, nPts, lambda0, lambda1, lambda2, error_relativo, costo_computacional, 
      modelo entrenado, historial, soluciÃ³n exacta, xList].
    """

    # ğŸ“Œ Determinar la cantidad de lotes basada en el nÃºmero de experimentos
    num_lotes = min(len(func), n_resultados)  
    lote_size = n_resultados // num_lotes  # ğŸ“Œ NÃºmero de experimentos por lote

 

    # ğŸ“Œ Lista para almacenar resultados
    resultados = []
    # ğŸ”¹  Se determinan los parÃ¡metros
    hiperparametros = hiperparametros_por_defecto_1()

    for i in range(num_lotes):
        # ğŸ”¹ Seleccionar la funciÃ³n de referencia y la soluciÃ³n exacta
        f_rhs, exactU = func[i]  
        print(f"\nğŸ”¹ Ejecutando Lote {i+1}...\n")

        for j in range(lote_size):
            experimento_id = i * lote_size + j + 1
            print(f"   â–¶ Ejecutando experimento {experimento_id}...")


            # ğŸ”¹ Entrenar la PINN con la configuraciÃ³n actual
            error_relativo, costo_computacional, history, uModel, xList = train_PINN(hiperparametros, f_rhs, exactU) #Esta bien.

            # ğŸ”¹ Imprimir resultados del experimento
            print(f"      ğŸ”¹ Completado - Error relativo: {error_relativo:.5f}, Uso CPU: {costo_computacional:.2f}%")

            # ğŸ“Œ Guardar los resultados del experimento
            resultados.append([
                experimento_id, 
                hiperparametros["neurons"], 
                hiperparametros["nLayers"], 
                hiperparametros["nPts"], 
                hiperparametros["lambda0"], 
                hiperparametros["lambda1"], 
                hiperparametros["lambda2"], 
                error_relativo, 
                costo_computacional,  
                uModel,   
                history,  
                exactU,   
                xList     
            ])

    return resultados

# ğŸ”¹ Definimos las funciones de referencia y sus soluciones exactas en un diccionario
funciones_experimentos = [
    {
        'fRhs': lambda x: (alpha + 4) * keras.ops.sin(2 * x),
        'exactU': lambda x: keras.ops.sin(2 * x),
        'title': 'sin(2x)'
    },
    {
        'fRhs': lambda x: (17/4) * keras.ops.sin(2 * x) * keras.ops.cos(x / 2) + 
                          2 * keras.ops.sin(x / 2) * keras.ops.cos(2 * x) + 
                          alpha * keras.ops.sin(2 * x) * keras.ops.cos(x / 2),
        'exactU': lambda x: keras.ops.sin(2 * x) * keras.ops.cos(x / 2),
        'title': 'sin(2x) * cos(x/2)'
    },
    {
        'fRhs': lambda x: -2 * (6 * x**2 - 6 * np.pi * x + np.pi**2) + 
                          alpha * (x**2 * (x - np.pi)**2),
        'exactU': lambda x: x**2 * (x - np.pi)**2,
        'title': 'x^2 * (x - pi)^2'
    }
]

# ğŸ”¹ Convertir el diccionario en una lista de tuplas
func = [(exp['fRhs'], exp['exactU']) for exp in funciones_experimentos]




def guardar_resultados_excel(resultados, funciones_experimentos, file_path="resultados/PLANTEAMIENTO 1/resultados.xlsx"):
    """
    Guarda los resultados de los entrenamientos en un archivo Excel, separando por lote.

    Cada hoja del archivo representa un conjunto de resultados de entrenamientos con la misma funciÃ³n de referencia.

    ParÃ¡metros:
    - resultados (lista): Lista de resultados obtenidos de `ejecutar_entrenamientos`.
    - funciones_experimentos (lista): Lista de funciones de referencia con su tÃ­tulo.
    - file_path (str, opcional): Ruta del archivo Excel donde se guardarÃ¡n los datos.

    Retorna:
    - None
    """

    # ğŸ“Œ Verificar si el archivo ya existe
    file_exists = os.path.exists(file_path)

    # ğŸ“Œ Configurar el modo de apertura del archivo Excel
    mode = 'a' if file_exists else 'w'

    # ğŸ“Œ Crear el writer con openpyxl si el archivo ya existe, de lo contrario, usar xlsxwriter
    engine = 'openpyxl' if file_exists else 'xlsxwriter'

    # ğŸ“Œ Abrir el archivo Excel
    with pd.ExcelWriter(file_path, engine=engine, mode=mode) as writer:
        
        # ğŸ“Œ Recorrer cada entrenamiento y guardarlo en su respectiva hoja
        for i, experimento in enumerate(funciones_experimentos):
            lote_resultados = [
                [r[j] for j in range(9)] for r in resultados if (r[0] - 1) // (len(resultados) // len(funciones_experimentos)) == i
            ]

            # ğŸ“Œ Convertir a DataFrame
            df = pd.DataFrame(
                lote_resultados,
                columns=["ID", "neurons", "nLayers", "nPts", 
                         "lambda0", "lambda1", "lambda2", "ERROR RELATIVO", "COSTO COMPUTACIONAL (CPU %)"]
            )

            # Limpiar y truncar el nombre de la hoja para evitar errores
            sheet_name = experimento['title']
            sheet_name = re.sub(r'[:\\/\*\[\]\?]', '', sheet_name)[:31]  # Elimina caracteres invÃ¡lidos y limita a 31 caracteres

            # Verificar si hay datos antes de escribir en Excel
            if not df.empty:
                df.to_excel(writer, sheet_name=sheet_name, index=False)
            else:
                print(f"âš ï¸ Advertencia: No hay datos para guardar en la hoja '{sheet_name}'.")

    print(f"âœ” Resultados guardados exitosamente en {file_path}.")

def plot_results(uModel, history, exactU, xList, exp_folder):
    """
    Genera y guarda las grÃ¡ficas de un experimento, asegurando que se use la funciÃ³n exacta correcta.
    """
    rcParams['font.family'] = 'serif'
    rcParams['font.size'] = 14
    rcParams['legend.fontsize'] = 12
    rcParams['mathtext.fontset'] = 'cm'
    rcParams['axes.labelsize'] = 14

    # ComparaciÃ³n de la soluciÃ³n exacta y la aproximada
    fig, ax = plt.subplots()
    plt.plot(xList, uModel(xList), color='b', label="u_approx")
    plt.plot(xList, exactU(xList), color='m', label="u_exact")  # âœ… Se asegura que usa la funciÃ³n exacta correcta
    plt.legend()
    ax.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "comparacion_u.png"), dpi=300)
    plt.close()

    # EvoluciÃ³n de la pÃ©rdida y error relativo
    fig, ax1 = plt.subplots()
    ax1.plot(history.history['loss'], color='g', label="Loss")
    ax1.set_xscale('log')
    ax1.set_yscale('log')
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Loss", color='g')

    ax2 = ax1.twinx()
    ax2.plot(history.history['loss'], color='b', label="Relative Error")  # ğŸ“Œ Se debe reemplazar por la mÃ©trica correcta
    ax2.set_ylabel("Relative Error", color='b')

    ax1.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "error_vs_loss.png"), dpi=300)
    plt.close()

    # Error relativo a lo largo de las Ã©pocas
    fig, ax = plt.subplots()
    plt.plot(history.history['loss'], color='b', label="Relative Error")  # ğŸ“Œ Se debe ajustar con la mÃ©trica real
    ax.set_xscale('log')
    plt.legend()
    ax.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "error_relativo.png"), dpi=300)
    plt.close()

def seleccionar_experimentos(resultados):
    """
    Selecciona los experimentos que se van a graficar y los organiza en una lista.

    ParÃ¡metros:
    - resultados (lista): Lista de resultados obtenidos en `ejecutar_entrenamientos`.

    Retorna:
    - todos_experimentos (lista): Lista de experimentos seleccionados para graficar.
    """

    # ğŸ“Œ Determinar la cantidad de lotes segÃºn los experimentos disponibles
    num_lotes = min(len(func), len(resultados))  
    lote_size = max(1, len(resultados) // num_lotes)  

    # ğŸ“Œ Lista para almacenar los experimentos seleccionados
    todos_experimentos = []

    for i in range(num_lotes):
        # ğŸ”¹ Seleccionar experimentos de este lote
        lote_resultados = [r for r in resultados if (r[0] - 1) // lote_size == i]
        todos_experimentos.extend(lote_resultados)

    print("âœ” Experimentos listos para generar grÃ¡ficas.")
    return todos_experimentos


def generar_graficas(experimentos, resultados, graficas_path):
    """
    Genera y guarda las grÃ¡ficas de los experimentos en una carpeta especÃ­fica.

    Cada experimento tendrÃ¡ su propia carpeta donde se guardarÃ¡n:
    - La comparaciÃ³n entre la soluciÃ³n exacta y la aproximada.
    - La evoluciÃ³n de la pÃ©rdida durante el entrenamiento.
    - La evoluciÃ³n del error relativo.
    - La comparaciÃ³n entre la pÃ©rdida y el error relativo.

    ParÃ¡metros:
    - experimentos (lista): Lista de experimentos seleccionados para graficar.
    - resultados (lista): Lista de resultados obtenidos en `ejecutar_entrenamientos`.
    - graficas_path (str): Ruta donde se almacenarÃ¡n las grÃ¡ficas.

    Retorna:
    - None
    """

    for exp in experimentos:
        exp_id = exp[0]  # ID del experimento
        exp_folder = os.path.join(graficas_path, f"Experimento_{exp_id}")
        os.makedirs(exp_folder, exist_ok=True)  # ğŸ“Œ Crear carpeta del experimento si no existe

        # ğŸ”¹ Extraer datos del experimento correspondiente
        uModel, history, exactU, xList = resultados[exp_id - 1][9:]

        # ğŸ“Œ Generar las grÃ¡ficas
        plot_results(uModel, history, exactU, xList, exp_folder)

    print(f"âœ” Todas las grÃ¡ficas generadas en {graficas_path}.")




# ğŸ”¹ Paso 1: ConfiguraciÃ³n inicial (Crear directorios y archivo Excel)
print("ğŸ“‚ Configurando estructura de almacenamiento...")
base_path, graficas_path = crear_estructura_directorios()
excel_file = inicializar_excel()

# ğŸ”¹ Paso 2: Ejecutar experimentos
print("ğŸš€ Ejecutando experimentos...")
resultados_experimentos = ejecutar_entrenamientos(func, 9)


# ğŸ”¹ Paso 3: Seleccionar los experimentos para graficar
print("ğŸ“Š Seleccionando experimentos para generaciÃ³n de grÃ¡ficas...")
experimentos_seleccionados = seleccionar_experimentos(resultados_experimentos)

# ğŸ”¹ Paso 4: Generar todas las grÃ¡ficas
print("ğŸ“ˆ Generando grÃ¡ficas...")
generar_graficas(experimentos_seleccionados, resultados_experimentos, graficas_path)

# ğŸ”¹ Paso 5: Guardar los resultados en Excel
print("ğŸ’¾ Guardando resultados en archivo Excel...")
guardar_resultados_excel(resultados_experimentos, funciones_experimentos)

print("âœ… Proceso finalizado exitosamente.")
