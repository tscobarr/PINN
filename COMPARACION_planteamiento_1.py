import tensorflow as tf  # Librer√≠a para aprendizaje profundo
import numpy as np  # Librer√≠a para manipulaci√≥n de matrices y c√°lculos num√©ricos
import matplotlib.pyplot as plt  # Librer√≠a para visualizaci√≥n de gr√°ficos
from matplotlib import rcParams  # Configuraci√≥n de gr√°ficos
import os  # Manejo de archivos y directorios
import pandas as pd  # Manejo de datos en formato tabular
os.environ["KERAS_BACKEND"] = "tensorflow"  # Define TensorFlow como backend de Keras
import keras_core as keras  # Librer√≠a de redes neuronales basada en TensorFlow
import time  # Para medir tiempos de ejecuci√≥n
import gc  # Para gesti√≥n de la memoria (recolecci√≥n de basura)
import psutil  # Para medir uso de CPU durante la ejecuci√≥n de experimentos
import time
import re 

# Configuraci√≥n de reproducibilidad
keras.utils.set_random_seed(1234)  # Fija una semilla para asegurar resultados reproducibles
dtype = 'float32'  # Establece la precisi√≥n de los c√°lculos
keras.backend.set_floatx(dtype)  # Aplica el tipo de dato definido

# Definici√≥n de par√°metros globales
alpha = 1  # Par√°metro utilizado en las ecuaciones diferenciales
limInf = 0  # L√≠mite inferior del dominio de la soluci√≥n
limSup = np.pi  # L√≠mite superior del dominio de la soluci√≥n
iterations = 3 # epocas por experimento


def makeModel(neurons, nLayers, activation):
    """
    Construye un modelo de red neuronal profunda basado en una PINN (Physics-Informed Neural Network).
    
    Par√°metros:
    - neurons (int): N√∫mero de neuronas en cada capa oculta.
    - nLayers (int): N√∫mero total de capas en la red (incluyendo la capa de entrada y salida).
    - activation (str): Funci√≥n de activaci√≥n utilizada en las capas ocultas.

    Retorna:
    - model (keras.Model): Modelo de red neuronal compilado.
    """
    # Capa de entrada con una √∫nica neurona para recibir valores de 'x'
    xVals = keras.layers.Input(shape=(1,), name='x_input', dtype='float32')

    # Primera capa oculta
    l1 = keras.layers.Dense(neurons, activation=activation, dtype='float32')(xVals)

    # Capas ocultas intermedias (se asegura que haya al menos una capa oculta)
    for _ in range(max(1, nLayers - 2)):  
        l1 = keras.layers.Dense(neurons, activation=activation, dtype='float32')(l1)

    # Capa de salida con una √∫nica neurona (salida escalar)
    output = keras.layers.Dense(1, activation=activation, dtype='float32')(l1)

    # Construcci√≥n del modelo
    return keras.Model(inputs=xVals, outputs=output, name='u_model')
class Loss1(keras.layers.Layer):
    def __init__(self, uModel, nPts, f, lambda0=1, lambda1=1, lambda2=1, limInf=0, limSup=np.pi, A=0, B=0, **kwargs):
        super(Loss1, self).__init__()
        self.uModel = uModel
        self.nPts = nPts
        self.f = f
        self.lambda0 = lambda0
        self.lambda1 = lambda1
        self.lambda2 = lambda2
        self.limInf = limInf
        self.limSup = limSup
        self.A = A
        self.B = B

    def call(self, inputs):
        """
        Eval√∫a la funci√≥n de p√©rdida en el dominio y los bordes.
        """

        # üîπ Convertir la entrada `inputs` a `float32`
        x = tf.cast(inputs, tf.float32)  # ‚úÖ Convertimos expl√≠citamente a `float32`

        # üîπ Muestreo de puntos en el dominio con `float32`
        x_samples = tf.random.uniform([self.nPts], self.limInf, self.limSup, dtype=tf.float32)

        with tf.GradientTape(persistent=True) as t1:
            t1.watch(x_samples)
            with tf.GradientTape(persistent=True) as t2:
                t2.watch(x_samples)
                u = self.uModel(x_samples, training=True)  # ‚úÖ Se mantiene en `float32`
            dux = t2.gradient(u, x_samples)
        duxx = t1.gradient(dux, x_samples)

        # üîπ Asegurar `float32` en todos los c√°lculos
        alpha_cast = tf.cast(alpha, tf.float32)
        f_x = tf.cast(self.f(x_samples), tf.float32)

        # üîπ C√°lculo del error de la ecuaci√≥n diferencial
        errorPDE = self.lambda0 * keras.ops.mean(tf.cast((-duxx + alpha_cast * u - f_x) ** 2, tf.float32))

        # üîπ Condiciones de contorno con `float32`
        bc = self.lambda1 * keras.ops.mean((self.uModel(tf.cast([self.limInf], tf.float32)) - tf.cast(self.A, tf.float32)) ** 2) + \
            self.lambda2 * keras.ops.mean((self.uModel(tf.cast([self.limSup], tf.float32)) - tf.cast(self.B, tf.float32)) ** 2)

        # ‚úÖ Convertir la salida a tensor para evitar errores en Keras
        return tf.convert_to_tensor(errorPDE + bc, dtype=tf.float32)



class RelativeErrorCallback(tf.keras.callbacks.Callback):
    """
    Callback personalizado para calcular el error relativo en norma H¬π al final de cada √©poca.

    Esta implementaci√≥n mide la diferencia en la funci√≥n y su derivada, 
    proporcionando una mejor evaluaci√≥n de la precisi√≥n del modelo.

    Par√°metros:
    - uModel (keras.Model): Modelo de la PINN entrenado.
    - exactU (funci√≥n): Funci√≥n de la soluci√≥n exacta.
    - nPts (int): N√∫mero de puntos en el dominio para evaluaci√≥n.
    """

    def __init__(self, uModel, exactU, nPts):
        super().__init__()
        self.uModel = uModel
        self.exactU = exactU
        self.nPts = nPts

    def on_epoch_end(self, epoch, logs=None):
        """
        Se ejecuta al final de cada √©poca y calcula el error relativo en norma H¬π.
        """

        # üîπ Generamos puntos en el dominio
        Sval = tf.experimental.numpy.linspace(0., np.pi, num=self.nPts * 10, dtype=dtype)

        # üîπ Calculamos derivadas con `GradientTape`
        with tf.GradientTape(persistent=True) as t1:
            t1.watch(Sval)
            ueval_val = self.uModel(Sval)  # Soluci√≥n aproximada de la PINN
            u_x_val = t1.gradient(ueval_val, Sval)  # Derivada de la soluci√≥n aproximada
            u_e = self.exactU(Sval)  # Soluci√≥n exacta
            ue_x = t1.gradient(u_e, Sval)  # Derivada de la soluci√≥n exacta
        del t1  # Liberar memoria

        # üîπ C√°lculo del error relativo en norma H¬π
        errorH01 = tf.reduce_mean((ue_x - u_x_val) ** 2)
        norm_exact = tf.reduce_mean(ue_x ** 2)
        relative_error = errorH01 / norm_exact

        # üîπ Guardamos el error relativo en los logs del entrenamiento
        logs['relative_error'] = relative_error


def makeLossModel1(uModel, nPts, f, lambda0=1, lambda1=1, lambda2=1, limInf=0, limSup=np.pi, A=0, B=0,):
    """
    Crea un modelo de p√©rdida basado en la funci√≥n `Loss1` para entrenar la PINN.

    En lugar de entrenar la PINN directamente, este modelo de p√©rdida se construye para 
    minimizar la ecuaci√≥n diferencial y las condiciones de contorno en cada iteraci√≥n.

    Par√°metros:
    - uModel (keras.Model): Modelo de la PINN que se est√° entrenando.
    - nPts (int): N√∫mero de puntos de muestreo en el dominio.
    - f (funci√≥n): Funci√≥n de referencia f(x) en la ecuaci√≥n diferencial.
    - lambda0 (float, opcional): Peso del t√©rmino de la ecuaci√≥n diferencial (PDE). Por defecto, 1.
    - lambda1 (float, opcional): Peso de la condici√≥n de contorno en el l√≠mite inferior. Por defecto, 1.
    - lambda2 (float, opcional): Peso de la condici√≥n de contorno en el l√≠mite superior. Por defecto, 1.
    - limInf (float, opcional): L√≠mite inferior del dominio. Por defecto, 0.
    - limSup (float, opcional): L√≠mite superior del dominio. Por defecto, œÄ.

    Retorna:
    - lossModel (keras.Model): Modelo de p√©rdida para la PINN.
    """
    # Entrada ficticia para alimentar el modelo de p√©rdida
    xVals = keras.layers.Input(shape=(1,), name='x_input', dtype=tf.float32)

    # Aplicamos la capa personalizada `Loss1` que calcula la p√©rdida de la ecuaci√≥n diferencial
    loss_output = Loss1(uModel, nPts, f, lambda0, lambda1, lambda2, limInf, limSup,A, B,)(xVals)

    # Construcci√≥n del modelo de p√©rdida
    lossModel = keras.Model(inputs=xVals, outputs=loss_output)
    
    return lossModel

def trickyLoss(yPred, yTrue):
    """
    Funci√≥n de p√©rdida ficticia utilizada en el entrenamiento de la PINN.

    Dado que la p√©rdida real es calculada por la capa `Loss1`, esta funci√≥n 
    simplemente devuelve `yTrue` sin hacer ning√∫n c√°lculo. 

    Par√°metros:
    - yPred (tensor): Predicci√≥n del modelo de p√©rdida (`lossModel`).
    - yTrue (tensor): Etiqueta objetivo (se ignora).

    Retorna:
    - yTrue (tensor): Se devuelve sin modificaciones.
    """
    return yTrue


## INICIALIZAR CARPETAS Y EXCEL ## 


def crear_estructura_directorios(base_path="resultados/PLANTEAMIENTO 1"):
    """
    Crea la estructura de directorios necesarios para almacenar los resultados de los resultados.

    Par√°metros:
    - base_path (str): Ruta base donde se almacenar√°n los resultados.

    Retorna:
    - base_path (str): Ruta base de los experimentos.
    - graficas_path (str): Ruta donde se guardar√°n las gr√°ficas generadas.
    """

    try:
        # üìå Verificar si la ruta ya existe
        if os.path.exists(base_path):
            if os.path.isfile(base_path):
                print(f"‚ö†Ô∏è Advertencia: '{base_path}' es un archivo, no un directorio. Se eliminar√° y se crear√° como carpeta.")
                os.remove(base_path)  
                os.makedirs(base_path)  
            else:
                print(f"‚úî La carpeta '{base_path}' ya existe.")
        else:
            os.makedirs(base_path)  

        # üìå Crear subdirectorio para gr√°ficas
        graficas_path = os.path.join(base_path, "graficas")
        os.makedirs(graficas_path, exist_ok=True)

        print("‚úî Directorios creados correctamente.")
        return base_path, graficas_path

    except Exception as e:
        print(f"‚ùå Error al crear directorios: {e}")
        return None, None  # Retornar None en caso de error


def inicializar_excel(file_path="resultados/PLANTEAMIENTO 1/resultados.xlsx"):
    """
    Crea un archivo Excel con la estructura inicial si no existe. 
    El archivo contendr√° tres hojas llamadas "Lote 1", "Lote 2" y "Lote 3",
    donde se almacenar√°n los resultados de los experimentos.

    Par√°metros:
    - file_path (str): Ruta del archivo Excel donde se guardar√°n los datos.

    Retorna:
    - file_path (str): Ruta del archivo Excel.
    """
    if not os.path.exists(file_path):
        writer = pd.ExcelWriter(file_path, engine='xlsxwriter')
        for i in range(1, 4):
            # Definir las columnas del archivo Excel
            df = pd.DataFrame(columns=[
                "Experimento", "neurons", "nLayers", "nPts", 
                "lambda0", "lambda1", "lambda2", "ERROR RELATIVO", "COSTO COMPUTACIONAL"
            ])
        writer.close()
        print("‚úî Archivo Excel inicializado.")
    else:
        print("‚úî Archivo Excel ya existe.")
    
    return file_path


# Funci√≥n para obtener los hiperpar√°metros por defecto
def hiperparametros_por_defecto_1():
    """
    Devuelve un diccionario con valores optimos para los hiperpar√°metros de la PINN que fueron determinados
    experimentalmente.

    Retorna:
    - dict: Diccionario con los hiperpar√°metros iniciales.
    """
    return {
        "neurons": 8,  # N√∫mero de neuronas por capa
        "nLayers": 5,  # N√∫mero de capas ocultas
        "nPts": 1608,  # N√∫mero de puntos en el dominio
        "lambda0": 1,  # Peso del error de la ecuaci√≥n diferencial
        "lambda1": 5,  # Peso del error en el contorno inferior
        "lambda2": 4   # Peso del error en el contorno superior
    }


def medir_costo_computacional(funcion_entrenamiento, *args, **kwargs):
    """
    Esta funci√≥n mide el porcentaje de uso de CPU y el tiempo total transcurrido durante la ejecuci√≥n de una funci√≥n espec√≠fica. 
    Su prop√≥sito es evaluar el costo computacional de un proceso en t√©rminos de utilizaci√≥n del procesador.

    Para lograrlo, la funci√≥n realiza las siguientes mediciones antes y despu√©s de la ejecuci√≥n de la funci√≥n objetivo:
    1. **Tiempo de CPU utilizado**: Representa el tiempo total que la CPU ha dedicado activamente a ejecutar la funci√≥n, 
    sin contar periodos de inactividad o espera.
    2. **Tiempo real transcurrido**: Es el tiempo total que ha pasado desde el inicio hasta el final de la ejecuci√≥n de la funci√≥n.
    3. **N√∫mero de n√∫cleos l√≥gicos del procesador**: Determina la capacidad total del sistema para distribuir la carga de trabajo.

    El c√°lculo del uso de CPU se realiza dividiendo el tiempo de CPU utilizado entre el producto del tiempo real transcurrido 
    y el n√∫mero de n√∫cleos l√≥gicos. Esto permite normalizar la medici√≥n para obtener un valor porcentual que refleja 
    qu√© tanto del poder total del procesador fue utilizado durante la ejecuci√≥n.


    Par√°metros:
    - funcion_entrenamiento (funci√≥n): La funci√≥n que queremos medir.
    - *args: Argumentos posicionales de la funci√≥n.
    - **kwargs: Argumentos con nombre de la funci√≥n.

    Retorna:
    - cpu_percent (float): Porcentaje medio de CPU usado durante la ejecuci√≥n.
    - elapsed_time (float): Tiempo total transcurrido en segundos.
        
    """



    process = psutil.Process(os.getpid())  # Obtener el proceso actual

    # üîπ Medir el tiempo de CPU y tiempo real antes del entrenamiento
    cpu_times_before = process.cpu_times()
    time_before = time.time()

    # üîπ Ejecutar la funci√≥n de entrenamiento con sus argumentos
    funcion_entrenamiento(*args, **kwargs)

    # üîπ Medir el tiempo de CPU y tiempo real despu√©s del entrenamiento
    cpu_times_after = process.cpu_times()
    time_after = time.time()

    # üîπ Calcular tiempo total de CPU consumido
    cpu_time_used = (cpu_times_after.user + cpu_times_after.system) - (cpu_times_before.user + cpu_times_before.system)

    # üîπ Calcular tiempo real transcurrido
    elapsed_time = time_after - time_before

    # üîπ Calcular porcentaje de CPU usado
    num_cpus = psutil.cpu_count(logical=True)  # N√∫mero de n√∫cleos l√≥gicos

    cpu_percent = (cpu_time_used / (elapsed_time * num_cpus)) * 100 if elapsed_time > 0 else 0 

    return cpu_percent, elapsed_time


def train_PINN(hiperparametros, funcion_referencia, funcion_exacta):
    """
    Entrena la PINN con los hiperpar√°metros dados y la funci√≥n de referencia seleccionada.

    Tambi√©n mide el error relativo y el costo computacional en t√©rminos de uso de CPU.

    Par√°metros:
    - hiperparametros (dict): Diccionario con la configuraci√≥n de la red (neurons, nLayers, etc.).
    - funcion_referencia (funci√≥n): Funci√≥n f(x) en la ecuaci√≥n diferencial que la PINN debe aprender.
    - funcion_exacta (funci√≥n): Soluci√≥n exacta esperada para evaluar el error relativo.

    Retorna:
    - error_relativo (float): Error relativo entre la soluci√≥n de la PINN y la soluci√≥n exacta.
    - costo_computacional (float): Uso de CPU en porcentaje durante el entrenamiento.
    - history (tf.keras.callbacks.History): Historial del entrenamiento.
    - uModel (keras.Model): Modelo entrenado de la PINN.
    - xList (array): Puntos en el dominio donde se evalu√≥ la soluci√≥n.
    """

    # üîπ Medir tiempo de CPU antes del entrenamiento
    process = psutil.Process(os.getpid())

    # üîπ Construcci√≥n del modelo de red neuronal PINN
    uModel = makeModel(
        neurons=hiperparametros["neurons"],
        nLayers=hiperparametros["nLayers"],
        activation='tanh'
    )

    # üìå Calcular valores exactos en los bordes (x = 0 y x = œÄ)
    A = 0 #funcion_exacta(tf.convert_to_tensor(0.0, dtype=dtype))  # üìå Se asegura de que x sea un tensor
    B = 0 #funcion_exacta(tf.convert_to_tensor(np.pi, dtype=dtype)) 

    # üîπ Definir el modelo de p√©rdida basado en la ecuaci√≥n diferencial
    lossModel = makeLossModel1(
        uModel,
        hiperparametros["nPts"],
        funcion_referencia,
        hiperparametros["lambda0"],
        hiperparametros["lambda1"],
        hiperparametros["lambda2"],
        limInf, limSup,
        A, B  # üìå Se pasan los valores exactos en los extremos
    )

    # üìå Configurar el optimizador y la funci√≥n de p√©rdida
    optimizer = keras.optimizers.Adam(learning_rate=1e-3)
    lossModel.compile(optimizer=optimizer, loss=trickyLoss)

    relative_error_callback = RelativeErrorCallback(uModel, funcion_exacta, hiperparametros["nPts"]) #CAMBIO REALIZADO
    
    # üìå Entrenamiento de la PINN
    history = lossModel.fit(
        np.array([1.]), np.array([1.]), epochs=iterations, verbose=0, callbacks=[relative_error_callback]
    )


    # üîπ Generar puntos en el dominio para evaluar la soluci√≥n entrenada
    xList = np.array([np.pi / 1000 * i for i in range(1000)])

    # üîπ Evaluar la soluci√≥n aproximada y compararla con la soluci√≥n exacta
    error_relativo = history.history['relative_error'][-1].numpy()

    # üîπ Calcular tiempo total de CPU usado (sumando usuario y sistema)
    cpu_cost, _ = medir_costo_computacional(
        lossModel.fit, np.array([1.]), np.array([1.]), epochs=iterations, verbose=0, callbacks=[relative_error_callback]
    )

    # üîπ Liberar memoria
    keras.backend.clear_session()
    gc.collect()

    return error_relativo, cpu_cost, history, uModel, xList

def ejecutar_entrenamientos(func, n_resultados=6):  
    """
    Ejecuta m√∫ltiples veces el entrenamiento de la PINN con hiperpar√°metros √≥ptimos.

    Se generan los resultados de la PINN despu√©s de cada entrenamiento.

    Par√°metros:
    - func (lista de tuplas): Lista de funciones de referencia y soluciones exactas esperadas.
      Cada elemento es una tupla (funci√≥n_f, funci√≥n_u_exacta).
    - n_resultados (int, opcional): N√∫mero total de entrenamientos a ejecutar. Por defecto, 45.

    Retorna:
    - resultados (lista): Lista de resultados de los entrenamientos, donde cada elemento contiene:
      [ID, neurons, nLayers, nPts, lambda0, lambda1, lambda2, error_relativo, costo_computacional, 
      modelo entrenado, historial, soluci√≥n exacta, xList].
    """

    # üìå Determinar la cantidad de lotes basada en el n√∫mero de experimentos
    num_lotes = min(len(func), n_resultados)  
    lote_size = n_resultados // num_lotes  # üìå N√∫mero de experimentos por lote

 

    # üìå Lista para almacenar resultados
    resultados = []
    # üîπ  Se determinan los par√°metros
    hiperparametros = hiperparametros_por_defecto_1()

    for i in range(num_lotes):
        # üîπ Seleccionar la funci√≥n de referencia y la soluci√≥n exacta
        f_rhs, exactU = func[i]  
        print(f"\nüîπ Ejecutando Lote {i+1}...\n")

        for j in range(lote_size):
            experimento_id = i * lote_size + j + 1
            print(f"   ‚ñ∂ Ejecutando experimento {experimento_id}...")


            # üîπ Entrenar la PINN con la configuraci√≥n actual
            error_relativo, costo_computacional, history, uModel, xList = train_PINN(hiperparametros, f_rhs, exactU) #Esta bien.

            # üîπ Imprimir resultados del experimento
            print(f"      üîπ Completado - Error relativo: {error_relativo:.5f}, Uso CPU: {costo_computacional:.2f}%")

            # üìå Guardar los resultados del experimento
            resultados.append([
                experimento_id, 
                hiperparametros["neurons"], 
                hiperparametros["nLayers"], 
                hiperparametros["nPts"], 
                hiperparametros["lambda0"], 
                hiperparametros["lambda1"], 
                hiperparametros["lambda2"], 
                error_relativo, 
                costo_computacional,  
                uModel,   
                history,  
                exactU,   
                xList     
            ])

    return resultados

# üîπ Definimos las funciones de referencia y sus soluciones exactas en un diccionario
funciones_experimentos = [
    {
        'fRhs': lambda x: (alpha + 4) * keras.ops.sin(2 * x),
        'exactU': lambda x: keras.ops.sin(2 * x),
        'title': 'sin(2x)'
    },
    {
        'fRhs': lambda x: (17/4) * keras.ops.sin(2 * x) * keras.ops.cos(x / 2) + 
                          2 * keras.ops.sin(x / 2) * keras.ops.cos(2 * x) + 
                          alpha * keras.ops.sin(2 * x) * keras.ops.cos(x / 2),
        'exactU': lambda x: keras.ops.sin(2 * x) * keras.ops.cos(x / 2),
        'title': 'sin(2x) * cos(x/2)'
    },
    {
        'fRhs': lambda x: -2 * (6 * x**2 - 6 * np.pi * x + np.pi**2) + 
                          alpha * (x**2 * (x - np.pi)**2),
        'exactU': lambda x: x**2 * (x - np.pi)**2,
        'title': 'x^2 * (x - pi)^2'
    }
]

# üîπ Convertir el diccionario en una lista de tuplas
func = [(exp['fRhs'], exp['exactU']) for exp in funciones_experimentos]




def guardar_resultados_excel(resultados, funciones_experimentos, file_path="resultados/PLANTEAMIENTO 1/resultados.xlsx"):
    """
    Guarda los resultados de los entrenamientos en un archivo Excel, separando por lote.

    Cada hoja del archivo representa un conjunto de resultados de entrenamientos con la misma funci√≥n de referencia.

    Par√°metros:
    - resultados (lista): Lista de resultados obtenidos de `ejecutar_entrenamientos`.
    - funciones_experimentos (lista): Lista de funciones de referencia con su t√≠tulo.
    - file_path (str, opcional): Ruta del archivo Excel donde se guardar√°n los datos.

    Retorna:
    - None
    """

    # üìå Verificar si el archivo ya existe
    file_exists = os.path.exists(file_path)

    # üìå Configurar el modo de apertura del archivo Excel
    mode = 'a' if file_exists else 'w'

    # üìå Crear el writer con openpyxl si el archivo ya existe, de lo contrario, usar xlsxwriter
    engine = 'openpyxl' if file_exists else 'xlsxwriter'

    # üìå Abrir el archivo Excel
    with pd.ExcelWriter(file_path, engine=engine, mode=mode) as writer:
        
        # üìå Recorrer cada entrenamiento y guardarlo en su respectiva hoja
        for i, experimento in enumerate(funciones_experimentos):
            lote_resultados = [
                [r[j] for j in range(9)] for r in resultados if (r[0] - 1) // (len(resultados) // len(funciones_experimentos)) == i
            ]

            # üìå Convertir a DataFrame
            df = pd.DataFrame(
                lote_resultados,
                columns=["ID", "neurons", "nLayers", "nPts", 
                         "lambda0", "lambda1", "lambda2", "ERROR RELATIVO", "COSTO COMPUTACIONAL (CPU %)"]
            )

            # Limpiar y truncar el nombre de la hoja para evitar errores
            sheet_name = experimento['title']
            sheet_name = re.sub(r'[:\\/\*\[\]\?]', '', sheet_name)[:31]  # Elimina caracteres inv√°lidos y limita a 31 caracteres

            # Verificar si hay datos antes de escribir en Excel
            if not df.empty:
                df.to_excel(writer, sheet_name=sheet_name, index=False)
            else:
                print(f"‚ö†Ô∏è Advertencia: No hay datos para guardar en la hoja '{sheet_name}'.")

    print(f"‚úî Resultados guardados exitosamente en {file_path}.")

def plot_results(uModel, history, exactU, xList, exp_folder):
    """
    Genera y guarda las gr√°ficas de un experimento, asegurando que se use la funci√≥n exacta correcta.
    """
    rcParams['font.family'] = 'serif'
    rcParams['font.size'] = 14
    rcParams['legend.fontsize'] = 12
    rcParams['mathtext.fontset'] = 'cm'
    rcParams['axes.labelsize'] = 14

    # Comparaci√≥n de la soluci√≥n exacta y la aproximada
    fig, ax = plt.subplots()
    plt.plot(xList, uModel(xList), color='b', label="u_approx")
    plt.plot(xList, exactU(xList), color='m', label="u_exact")  # ‚úÖ Se asegura que usa la funci√≥n exacta correcta
    plt.legend()
    ax.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "comparacion_u.png"), dpi=300)
    plt.close()

    # Evoluci√≥n de la p√©rdida y error relativo
    fig, ax1 = plt.subplots()
    ax1.plot(history.history['loss'], color='g', label="Loss")
    ax1.set_xscale('log')
    ax1.set_yscale('log')
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Loss", color='g')

    ax2 = ax1.twinx()
    ax2.plot(history.history['loss'], color='b', label="Relative Error")  # üìå Se debe reemplazar por la m√©trica correcta
    ax2.set_ylabel("Relative Error", color='b')

    ax1.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "error_vs_loss.png"), dpi=300)
    plt.close()

    # Error relativo a lo largo de las √©pocas
    fig, ax = plt.subplots()
    plt.plot(history.history['loss'], color='b', label="Relative Error")  # üìå Se debe ajustar con la m√©trica real
    ax.set_xscale('log')
    plt.legend()
    ax.grid(which='both', linestyle=':')
    plt.tight_layout()
    plt.savefig(os.path.join(exp_folder, "error_relativo.png"), dpi=300)
    plt.close()

def seleccionar_experimentos(resultados):
    """
    Selecciona los experimentos que se van a graficar y los organiza en una lista.

    Par√°metros:
    - resultados (lista): Lista de resultados obtenidos en `ejecutar_entrenamientos`.

    Retorna:
    - todos_experimentos (lista): Lista de experimentos seleccionados para graficar.
    """

    # üìå Determinar la cantidad de lotes seg√∫n los experimentos disponibles
    num_lotes = min(len(func), len(resultados))  
    lote_size = max(1, len(resultados) // num_lotes)  

    # üìå Lista para almacenar los experimentos seleccionados
    todos_experimentos = []

    for i in range(num_lotes):
        # üîπ Seleccionar experimentos de este lote
        lote_resultados = [r for r in resultados if (r[0] - 1) // lote_size == i]
        todos_experimentos.extend(lote_resultados)

    print("‚úî Experimentos listos para generar gr√°ficas.")
    return todos_experimentos


def generar_graficas(experimentos, resultados, graficas_path):
    """
    Genera y guarda las gr√°ficas de los experimentos en una carpeta espec√≠fica.

    Cada experimento tendr√° su propia carpeta donde se guardar√°n:
    - La comparaci√≥n entre la soluci√≥n exacta y la aproximada.
    - La evoluci√≥n de la p√©rdida durante el entrenamiento.
    - La evoluci√≥n del error relativo.
    - La comparaci√≥n entre la p√©rdida y el error relativo.

    Par√°metros:
    - experimentos (lista): Lista de experimentos seleccionados para graficar.
    - resultados (lista): Lista de resultados obtenidos en `ejecutar_entrenamientos`.
    - graficas_path (str): Ruta donde se almacenar√°n las gr√°ficas.

    Retorna:
    - None
    """

    for exp in experimentos:
        exp_id = exp[0]  # ID del experimento
        exp_folder = os.path.join(graficas_path, f"Experimento_{exp_id}")
        os.makedirs(exp_folder, exist_ok=True)  # üìå Crear carpeta del experimento si no existe

        # üîπ Extraer datos del experimento correspondiente
        uModel, history, exactU, xList = resultados[exp_id - 1][9:]

        # üìå Generar las gr√°ficas
        plot_results(uModel, history, exactU, xList, exp_folder)

    print(f"‚úî Todas las gr√°ficas generadas en {graficas_path}.")




# üîπ Paso 1: Configuraci√≥n inicial (Crear directorios y archivo Excel)
print("üìÇ Configurando estructura de almacenamiento...")
base_path, graficas_path = crear_estructura_directorios()
excel_file = inicializar_excel()

# üîπ Paso 2: Ejecutar experimentos
print("üöÄ Ejecutando experimentos...")
resultados_experimentos = ejecutar_entrenamientos(func, 9)


# üîπ Paso 3: Seleccionar los experimentos para graficar
print("üìä Seleccionando experimentos para generaci√≥n de gr√°ficas...")
experimentos_seleccionados = seleccionar_experimentos(resultados_experimentos)

# üîπ Paso 4: Generar todas las gr√°ficas
print("üìà Generando gr√°ficas...")
generar_graficas(experimentos_seleccionados, resultados_experimentos, graficas_path)

# üîπ Paso 5: Guardar los resultados en Excel
print("üíæ Guardando resultados en archivo Excel...")
guardar_resultados_excel(resultados_experimentos, funciones_experimentos)

print("‚úÖ Proceso finalizado exitosamente.")
